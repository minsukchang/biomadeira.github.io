---
layout: layout
---
<div class="container">
    <div class="row">
        <div class="col-md-3" style="text-align:center;">
            <div class="avatar">
                <img class="rounded-circle" src="{{ site.baseurl }}static/img/{{ site.avatar }}" alt="Responsive image" Height="150">
            </div>
            <h3>Minsuk Chang</h3>
            {% include social.html %}
        </div>
        <div class="col-md-9">
            <p><a> I'm a <a href="https://cs.kaist.ac.kr">Computer Science</a> PhD student in the <a href="https://kixlab.org"> KIXLAB</a> at <a href="https://www.kaist.ac.kr"> KAIST</a>
                advised by <a href="https://juhokim.com"> Juho Kim</a>. My work in HCI focuses on building interactive systems powered by statistical inference on large scale interaction data.
            </p>
            <p> 
            </p>
            <h5><strong> One-liner Research Objective: </strong></h5>
            <p> I develop techniques for discovering, capturing, and structuring user context in large-scale web data to create novel learning opportunities in the wild</p>

            <h5><strong>Latest Publications:</strong></h5>

            <div class="conference"> CHI 2019 </div>
            <div class="title"> How to Design Voice Based Navigation for How-To Videos</div>
            <div class="authors"> <i>Minsuk Chang</i>, Ahn Truong, Oliver Wang, Maneesh Agrawala, Juho Kim </div>
            <div class="abstract">
                    When watching how-to videos related to physical tasks,
                    users’ hands are often occupied by the task, making voice
                    input a natural fit. To better understand the design space of
                    voice interactions for how-to video navigation, we conducted
                    three think-aloud studies using: 1) a traditional video interface, 2) a research probe providing a voice controlled video
                    interface, and 3) a wizard-of-oz interface. From the studies,
                    we distill seven navigation objectives and their underlying
                    intents: pace control pause, content alignment pause, video
                    control pause, reference jump, replay jump, skip jump, and
                    peek jump. Our analysis found that users’ navigation objectives and intents affect the choice of referent type and
                    referencing approach in command utterances. Based on our
                    findings, we recommend to 1) support conversational strategies like sequence expansions and command queues, 2) allow
                    users to identify and refine their navigation objectives explicitly, and 3) support the seven interaction intents.
            </div>


            <p> , "<a href="https://www.kixlab.org/files/2019/chi2019-VoiceVideoNavigation-paper.pdf"> </a>", CHI 2019 (to appear) </p>
            <p> <i>Minsuk Chang</i>, Leonore V. Guillain, Hyeungshik Jung, Vivian M. Hare, Juho Kim, and Maneesh Agrawala. "<a href="https://recipescape.kixlab.org">RecipeScape: An Interactive Tool for Analyzing Cooking Instructions at Scale</a>", CHI 2018</p>
            <!-- <p> <i>Minsuk, Chang</i>, Vivian M. Hare, Juho Kim, and Maneesh Agrawala. "<a href="recipescape.kixlab.org">RecipeScape: Mining and Analyzing Diverse Processes in Cooking Recipes</a>", CHI LBW 2017</p> -->







            <h5><strong>What I do (in detail):</strong><button type="button" class="btn btn-link toggle-tex" data-toggle="collapse" data-target="#whatido"><span class="glyphicon glyphicon-collapse-down"></span> Open</button></h5>
            <div id="whatido" class="collapse">
                
                    <p>I develop interaction techniques for <b>mining context in large-scale web data</b> to create/support learning opportunities in the wild. 
                       I study <b>computational representations</b> to structure the collective contexts mined, based on which I <b>build interfaces</b> for expanding people's abilities to search, analyze and make decisions to improve individual and collective learning experiences.</p>

                    <p>I'm interested in the interaction bounds of interfaces, and the resulting strategic decisions users are forced to make. For example, I've looked at
                        <ul>
                            <li>users having to go back and forth between search results of thousand items and individual page of one specific item.</li>
                            <li>users deciding whether to get the task done as fast as possible or invest time to learn how to do the task better.</li>
                            <li>users having to use hands for both controlling the video tutorial and the task at hand.</li>
                        </ul>
                    </p>

                    <p>I build alternative systems to these computationally bounded interfaces by using techniques from interaction design, human computation, and machine learning. 
                        At the same time, I aim to leverage these situations to learn about <b>diverse explanations, reasonings, relationships between the choices users make</b> with a vision that they will not only expand what people can learn but also expand what machines can learn.</p>

                    <p>I also spend a lot of my time thinking about ways we can improve learning at scale (i.e. MOOC) with peer learning and with interactions that leverage peer dynamics.</p>

                    <p>I taught lab sessions for the mandatory Introduction to Programming course from 2015-2018 as a Head TA. 
                    I enjoyed working with 40 TAs and interacting with 450-500 students each semester.</p>
            </div>

            <h5><strong> How I got here:</strong><button type="button" class="btn btn-link toggle-tex" data-toggle="collapse" data-target="#cv"><span class="glyphicon glyphicon-collapse-down"></span> Open </button></h5>
            <div id="cv" class="collapse">
            <p>I studied Computer Science, Financial Engineering from KAIST, Finance from Simon Business School @ University of Rochester, and Statistics
                from Rutgers University. I have worked at an Hedge Fund in NYC trying to beat the market by relentlessly crunching numbers prior to coming (back) to KAIST.
                I've spent two years in the reinforcement learning (as a subfield of machine learning) research group at KAIST as a Ph.D student before joining KIXLAB (the KAIST Interaction Lab).
                <a href="{{ site.baseurl }}static/CV_minsukchang_2018_USLetter.pdf"> A more detailed CV (pdf).</a></p>
            </div>

            <h5><strong>Latest News:</strong></h5>
            {% include news.html limit=4 %}

        </div>
    </div>
</div>
